{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_zRJ0UdClog",
        "colab_type": "text"
      },
      "source": [
        "# Homework 4: Conversation Modeling and decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpYIpxHWCloi",
        "colab_type": "text"
      },
      "source": [
        "# Part 1 Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8bvMgCtCloj",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Attention visulization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcKqEr2oClol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### set up the model and complete the corresponding task\n",
        "\n",
        "### the pretrained model was trained in ~2 hours, i.e. you can expect attention maps\n",
        "### to look quite 'hard' (less soft spreading) i.e. attending to some particular token in the input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95JRn3fJGYvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJY-5KUeGYyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUvNXVlJGY1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatDictionary(object):\n",
        "    \"\"\"\n",
        "    Simple dict loader\n",
        "    \"\"\"\n",
        "    def __init__(self, dict_file_path):\n",
        "        self.word2ind = {}  # word:index\n",
        "        self.ind2word = {}  # index:word\n",
        "        self.counts = {}  # word:count\n",
        "\n",
        "        dict_raw = open(dict_file_path, 'r').readlines()\n",
        "        \n",
        "        for i, w in enumerate(dict_raw):\n",
        "            _word, _count = w.strip().split('\\t')\n",
        "            if _word == '\\\\n':\n",
        "                _word = '\\n'\n",
        "            self.word2ind[_word] = i\n",
        "            self.ind2word[i] = _word\n",
        "            self.counts[_word] = _count\n",
        "            \n",
        "    def t2v(self, tokenized_text):\n",
        "        return [self.word2ind[w] if w in self.counts else self.word2ind['__unk__'] for w in tokenized_text]\n",
        "\n",
        "    def v2t(self, list_ids):\n",
        "        return ' '.join([self.ind2word[i] for i in list_ids])\n",
        "    \n",
        "    def pred2text(self, tensor):\n",
        "        result = []\n",
        "        for i in range(tensor.size(0)):\n",
        "            if tensor[i].item() == '__end__'  or tensor[i].item() == '__null__':  # null is pad\n",
        "                break\n",
        "            else:\n",
        "                result.append(self.ind2word[tensor[i].item()])\n",
        "        return ' '.join(result)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4EJ0_xeGY3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ChatDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Json dataset wrapper\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_file_path, dictionary, dt='train'):\n",
        "        super().__init__()\n",
        "        \n",
        "        json_text = open(dataset_file_path, 'r').readlines()\n",
        "        self.samples = []\n",
        "        \n",
        "        for sample in tqdm(json_text):\n",
        "            sample = sample.rstrip()\n",
        "            sample = json.loads(sample)\n",
        "            _inp_toked = RETOK.findall(sample['text'])\n",
        "            _inp_toked_id = dictionary.t2v(_inp_toked)\n",
        "\n",
        "            sample['text_vec'] = torch.tensor(_inp_toked_id, dtype=torch.long)\n",
        "            \n",
        "            # train and valid have different key names for target\n",
        "            if dt == 'train':\n",
        "                _tar_toked = RETOK.findall(sample['labels'][0]) + ['__end__']\n",
        "            elif dt == 'valid':\n",
        "                _tar_toked = RETOK.findall(sample['eval_labels'][0]) + ['__end__']\n",
        "                \n",
        "            _tar_toked_id = dictionary.t2v(_tar_toked)\n",
        "            \n",
        "            sample['target_vec'] = torch.tensor(_tar_toked_id, dtype=torch.long)\n",
        "            \n",
        "            self.samples.append(sample)\n",
        "            \n",
        "    def __getitem__(self, i):\n",
        "        return self.samples[i]['text_vec'], self.samples[i]['target_vec']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT00w7kBGY6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_tensor(tensors, sort=True, pad_token=0):\n",
        "    rows = len(tensors)\n",
        "    lengths = [len(i) for i in tensors]\n",
        "    max_t = max(lengths)\n",
        "        \n",
        "    output = tensors[0].new(rows, max_t)\n",
        "    output.fill_(pad_token)  # 0 is a pad token here\n",
        "    \n",
        "    for i, (tensor, length) in enumerate(zip(tensors, lengths)):\n",
        "        output[i,:length] = tensor\n",
        "\n",
        "    return output, lengths\n",
        "\n",
        "def argsort(keys, *lists, descending=False):\n",
        "    \"\"\"Reorder each list in lists by the (descending) sorted order of keys.\n",
        "    :param iter keys: Keys to order by.\n",
        "    :param list[list] lists: Lists to reordered by keys's order.\n",
        "                             Correctly handles lists and 1-D tensors.\n",
        "    :param bool descending: Use descending order if true.\n",
        "    :returns: The reordered items.\n",
        "    \"\"\"\n",
        "    ind_sorted = sorted(range(len(keys)), key=lambda k: keys[k])\n",
        "    if descending:\n",
        "        ind_sorted = list(reversed(ind_sorted))\n",
        "    output = []\n",
        "    for lst in lists:\n",
        "        if isinstance(lst, torch.Tensor):\n",
        "            output.append(lst[ind_sorted])\n",
        "        else:\n",
        "            output.append([lst[i] for i in ind_sorted])\n",
        "    return output\n",
        "\n",
        "def batchify(batch):\n",
        "    inputs = [i[0] for i in batch]\n",
        "    labels = [i[1] for i in batch]\n",
        "    \n",
        "    input_vecs, input_lens = pad_tensor(inputs)\n",
        "    label_vecs, label_lens = pad_tensor(labels)\n",
        "    \n",
        "    # sort only wrt inputs here for encoder packinng\n",
        "    input_vecs, input_lens, label_vecs, label_lens = argsort(input_lens, input_vecs, input_lens, label_vecs, label_lens, descending=True)\n",
        "\n",
        "    return {\n",
        "        \"text_vecs\": input_vecs,\n",
        "        \"text_lens\": input_lens,\n",
        "        \"target_vecs\": label_vecs,\n",
        "        \"target_lens\": label_lens,\n",
        "        'use_packed': True\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuzWYnP2GY8w",
        "colab_type": "code",
        "outputId": "84f294e2-1344-4bb8-f8fe-df00658e583b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# loading datasets and dictionary\n",
        "\n",
        "# downloading pretrained models and data\n",
        "\n",
        "### DOWNLOADING THE FILES\n",
        "import os\n",
        "\n",
        "### persona chat dataset\n",
        "if not os.path.exists('./dict'):\n",
        "    !wget \"https://nyu.box.com/shared/static/sj9f87tofpicll89xbc154pmbztu5q4h\" -O './dict'\n",
        "if not os.path.exists('./train.jsonl'):\n",
        "    !wget \"https://nyu.box.com/shared/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\" -O './train.jsonl'\n",
        "if not os.path.exists('./valid.jsonl'):\n",
        "    !wget \"https://nyu.box.com/shared/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\" -O './valid.jsonl'\n",
        "\n",
        "if not os.path.exists('./chat_model_best_22.pt'):\n",
        "    !wget \"https://nyu.box.com/shared/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\" -O './chat_model_best_22.pt'\n",
        "\n",
        "chat_dict = ChatDictionary('./dict')\n",
        "train_dataset = ChatDataset('./train.jsonl', chat_dict)\n",
        "valid_dataset = ChatDataset('./valid.jsonl', chat_dict, 'valid')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-10 00:17:14--  https://nyu.box.com/shared/static/sj9f87tofpicll89xbc154pmbztu5q4h\n",
            "Resolving nyu.box.com (nyu.box.com)... 185.235.236.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/sj9f87tofpicll89xbc154pmbztu5q4h [following]\n",
            "--2019-11-10 00:17:19--  https://nyu.box.com/public/static/sj9f87tofpicll89xbc154pmbztu5q4h\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/sj9f87tofpicll89xbc154pmbztu5q4h [following]\n",
            "--2019-11-10 00:17:20--  https://nyu.app.box.com/public/static/sj9f87tofpicll89xbc154pmbztu5q4h\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 185.235.236.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!D5f2Rl3_KN2N3LOdHlac1DFld1AeMAJVSTWKXtTGDancOYiiVpCQP0TjJgCBkiuzstoUxjt_ehUb0Ht62l6xHmd6l1SZELpqQVGWwxtGVuTAhFLt4wtCD33HrvFUupOdM5UfoKCBP1ak6RT2yXlu7D0tEkOdFMNRIDpEPZyynNKMlu5H9p1oSGIYIg-hvkF2fvemY-5kTT0eXAwHL8wTGeg0FVcQJxRvQLJ-se_YQ4fKIF4akD7BstQc3m9t8xes_BRRVKQCVxy06us4xLSk_akd6OA9xj0y1-_i_jVoHxocjT3wROjv-Cz7zFTKsddr_LuBPFmNSjtWMrOU8gfXwDSTi8EIx7JLXx4ytRM8Qs_3pzHznBFOakEgaVIUU8jL6psOivGAO643x1s-w5FOzLByBNvPC7ZY2ZL6UmY6YA1BWOMSFKudja2k0rdLObD-vn4UV-8PiAQ_l0m8USmPZN0-g5YsTpPDyo_bEHVItnGS7KDs1Xjlbe-3Ivew64Cia1AAjF7Lo-Q-5tVxGD1tTY0c3Z8dIZ1gapdVoKMQtAt2_84gIuDyLXyuQPyiHZEjGkVejXQYThA-cW9oMr1AFApBxfnAScGKcsA2aeTh1hm0gHt0lPMAYCBlRHzSdnm2KUEuTMmaOqv2uNLOGQsrv-beGS-YjIwbEOmctp2QGA-RsPlXOPYFBQIuARazdded-dfu7S8pgfQF3YmYwQHCPH-1g3aAkhXumNqpIGE8TQoFa4OKKkf_KNwVMGah2jgcP5TxKd3uNaklwe3UxFfMceMkvO9QEzT2sPbO9Jb56SxsR9qMKKayBdrIDTItEmVk9ui0QKt9dewiFFADjC5tK2g0h4GCdzaYetH6QkKXxRJ7i7yt9vmizo1-WZmVlxcma12-R-h_oBusnAl5ZC5Xzk0fdIpj3VhT1KQZvNhoheIi0FlkWXWIKVvNzH2AlWkldPWD25tu8pi0N5CnhrW3nUaew2FuzjR_7asMeQRZjT6Bx25F4nB7sFMyWScjwPi9W6-v79XMLorrda6LD4nLyz5ijfH6qSnR-qFKphs2zbyVFgu0ECLe-VH-vK4dnynGXc7lp03DfL7hd5MSRAnZxHWhMvWgyr0dxC6xIMp7A7a05Kf7BoyKVIN69szncAF3WbHTLgpuXoKA63Q5BK9p8xGKc9YcBc8ix0oOjGDvI_XPZoAvddMv99Jxq6bA7O-PYQwU0jNd2WMerAjGdlVU8bUAJEKw9zU8zkP_OD3Q-HVdT-iSwaiAEeOXuHFvRXdwxY6jfsn_N_n_vIUlunmJ0fx-JyQhEDegaHduRNaaZ4dyuClMwlGELfqkAZ_lTl00-VFYJUoRmKhY0nz1TEA4qXns8oNjTD8ghObb-CxaTFx01g../download [following]\n",
            "--2019-11-10 00:17:20--  https://public.boxcloud.com/d/1/b1!D5f2Rl3_KN2N3LOdHlac1DFld1AeMAJVSTWKXtTGDancOYiiVpCQP0TjJgCBkiuzstoUxjt_ehUb0Ht62l6xHmd6l1SZELpqQVGWwxtGVuTAhFLt4wtCD33HrvFUupOdM5UfoKCBP1ak6RT2yXlu7D0tEkOdFMNRIDpEPZyynNKMlu5H9p1oSGIYIg-hvkF2fvemY-5kTT0eXAwHL8wTGeg0FVcQJxRvQLJ-se_YQ4fKIF4akD7BstQc3m9t8xes_BRRVKQCVxy06us4xLSk_akd6OA9xj0y1-_i_jVoHxocjT3wROjv-Cz7zFTKsddr_LuBPFmNSjtWMrOU8gfXwDSTi8EIx7JLXx4ytRM8Qs_3pzHznBFOakEgaVIUU8jL6psOivGAO643x1s-w5FOzLByBNvPC7ZY2ZL6UmY6YA1BWOMSFKudja2k0rdLObD-vn4UV-8PiAQ_l0m8USmPZN0-g5YsTpPDyo_bEHVItnGS7KDs1Xjlbe-3Ivew64Cia1AAjF7Lo-Q-5tVxGD1tTY0c3Z8dIZ1gapdVoKMQtAt2_84gIuDyLXyuQPyiHZEjGkVejXQYThA-cW9oMr1AFApBxfnAScGKcsA2aeTh1hm0gHt0lPMAYCBlRHzSdnm2KUEuTMmaOqv2uNLOGQsrv-beGS-YjIwbEOmctp2QGA-RsPlXOPYFBQIuARazdded-dfu7S8pgfQF3YmYwQHCPH-1g3aAkhXumNqpIGE8TQoFa4OKKkf_KNwVMGah2jgcP5TxKd3uNaklwe3UxFfMceMkvO9QEzT2sPbO9Jb56SxsR9qMKKayBdrIDTItEmVk9ui0QKt9dewiFFADjC5tK2g0h4GCdzaYetH6QkKXxRJ7i7yt9vmizo1-WZmVlxcma12-R-h_oBusnAl5ZC5Xzk0fdIpj3VhT1KQZvNhoheIi0FlkWXWIKVvNzH2AlWkldPWD25tu8pi0N5CnhrW3nUaew2FuzjR_7asMeQRZjT6Bx25F4nB7sFMyWScjwPi9W6-v79XMLorrda6LD4nLyz5ijfH6qSnR-qFKphs2zbyVFgu0ECLe-VH-vK4dnynGXc7lp03DfL7hd5MSRAnZxHWhMvWgyr0dxC6xIMp7A7a05Kf7BoyKVIN69szncAF3WbHTLgpuXoKA63Q5BK9p8xGKc9YcBc8ix0oOjGDvI_XPZoAvddMv99Jxq6bA7O-PYQwU0jNd2WMerAjGdlVU8bUAJEKw9zU8zkP_OD3Q-HVdT-iSwaiAEeOXuHFvRXdwxY6jfsn_N_n_vIUlunmJ0fx-JyQhEDegaHduRNaaZ4dyuClMwlGELfqkAZ_lTl00-VFYJUoRmKhY0nz1TEA4qXns8oNjTD8ghObb-CxaTFx01g../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190210 (186K) [application/octet-stream]\n",
            "Saving to: ‘./dict’\n",
            "\n",
            "./dict              100%[===================>] 185.75K  1.16MB/s    in 0.2s    \n",
            "\n",
            "2019-11-10 00:17:21 (1.16 MB/s) - ‘./dict’ saved [190210/190210]\n",
            "\n",
            "--2019-11-10 00:17:22--  https://nyu.box.com/shared/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\n",
            "Resolving nyu.box.com (nyu.box.com)... 185.235.236.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl [following]\n",
            "--2019-11-10 00:17:22--  https://nyu.box.com/public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl [following]\n",
            "--2019-11-10 00:17:23--  https://nyu.app.box.com/public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 185.235.236.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!RhnAEvVoA-S5wsQq2guRG95x06pXfuB1QSsOyt-BzPteL7NbyY2bKyqNqvSz-Ogi7egM2l8iF5gHAwU9jns2dhYUNHAbxcXLFto5pq9XLfNGED8FEb2p9ZP8k5hYqRoM2usOS0I0c04odNeCkhunAobRdI9AGuUel2kzThXAXd6Y1rb7JZVGm1YVsyBgWmp79HjTW4rDbPpFYR5mJT-9jC9OU5GWMyGwltcI2AL13YsgKv1lgxA6XIOPZDjCDcp7kmvmoicXndvh64I_MXgEExBGUY0rpmJh6brIqhHxfHZxuzpHMkICieINXvEgTEJgHploa0jQccWZ031Uq7csE67MnoLHGoiPdAOgEyWKOBwzRo8Kj5X_ox3mJKDJ-zplRBfUOjQkyJF8JgXxBofj0pVpXwJtp3BaJEcrWjhq6rwExF5h7Lyiychf5Y4-T9uLkiLBClKvWfVkUKXFROqU0YGnNGTi_EaGhKhxZqchfXOTyipyvy6AWQc2oh2ccv4oWDPuOkyAeTbTiUGRAV1sXX7Eo1knaN5j2SGg2sH8VljB5J2bgzDWGWAXgS47A16nOv5Bl7Tr859KxDW8LQxbaXJQSFSZWH6346ov_wAnfHQWjHPp_nG93C0s_7fYgJwRxcvEJhz6ae4SzXcrZwnec17yeJCGZOCqrfVRUlGsDQfLRqkSx2_8IFANR2a7O-0ooxJHhjW5HOQ1k2mMCK9Hge8YNyPJ129z0GUENjEZVcEE_6smoB3EL-NsT8Kyp9r2yw2N38T7r1G9vHHHhTgsPFNjvGAExg7lqc-FTbEVf392Prkk9OZfG-jo9p6Rk73jhAlYt8FbFZ0mOjYTvLA9sma1p8LHcLeSk0_cTVb881pfWj0CTr0fib1zKEldHLhU-GcvINtBhRtjP46IIxzCD18W5reARPfU0oX_APTDHa2LeGCk-oXJlIsjgX-npBxamKfQBrW6_RBONYwbZ5xUCAXw88LGHVkN-FVOZ0OhtiVMoCr4J6Q4Au8s_XoHh8P811OtP7M6ReYSBKDHpJtBZmxGFW_LESMN3I9hwEoSizXqIAQmW0NVffDtJHWlfC2tVdmCLi_tPBTqs5_7B54hT5-GG5HYcOxDub5OG3r4m4SpiFXe3pUSZr27CI_h4DREu6atKBJdjOEPToLgoG6H8_qT6ASl8ylPh6vy5WFZGl9_r-NSDc_UXqBLHkOfLYn_bv6xy1u9N3MMubmsxVVnD0QEMcNUwT444nLLNIMGMHj18w3fO27SgKqn08b5iYU8_6A0iOgG0zdVLR7rYdennoJGTWCNVXekXyRHBJon4IMa-5c2v2RXxMyCTFp7CmurYCMF23YGfUwpNEcSgMkAGT3hDWUcd4wgcv8BPYJVIooRxQbekYto2NDYvw../download [following]\n",
            "--2019-11-10 00:17:23--  https://public.boxcloud.com/d/1/b1!RhnAEvVoA-S5wsQq2guRG95x06pXfuB1QSsOyt-BzPteL7NbyY2bKyqNqvSz-Ogi7egM2l8iF5gHAwU9jns2dhYUNHAbxcXLFto5pq9XLfNGED8FEb2p9ZP8k5hYqRoM2usOS0I0c04odNeCkhunAobRdI9AGuUel2kzThXAXd6Y1rb7JZVGm1YVsyBgWmp79HjTW4rDbPpFYR5mJT-9jC9OU5GWMyGwltcI2AL13YsgKv1lgxA6XIOPZDjCDcp7kmvmoicXndvh64I_MXgEExBGUY0rpmJh6brIqhHxfHZxuzpHMkICieINXvEgTEJgHploa0jQccWZ031Uq7csE67MnoLHGoiPdAOgEyWKOBwzRo8Kj5X_ox3mJKDJ-zplRBfUOjQkyJF8JgXxBofj0pVpXwJtp3BaJEcrWjhq6rwExF5h7Lyiychf5Y4-T9uLkiLBClKvWfVkUKXFROqU0YGnNGTi_EaGhKhxZqchfXOTyipyvy6AWQc2oh2ccv4oWDPuOkyAeTbTiUGRAV1sXX7Eo1knaN5j2SGg2sH8VljB5J2bgzDWGWAXgS47A16nOv5Bl7Tr859KxDW8LQxbaXJQSFSZWH6346ov_wAnfHQWjHPp_nG93C0s_7fYgJwRxcvEJhz6ae4SzXcrZwnec17yeJCGZOCqrfVRUlGsDQfLRqkSx2_8IFANR2a7O-0ooxJHhjW5HOQ1k2mMCK9Hge8YNyPJ129z0GUENjEZVcEE_6smoB3EL-NsT8Kyp9r2yw2N38T7r1G9vHHHhTgsPFNjvGAExg7lqc-FTbEVf392Prkk9OZfG-jo9p6Rk73jhAlYt8FbFZ0mOjYTvLA9sma1p8LHcLeSk0_cTVb881pfWj0CTr0fib1zKEldHLhU-GcvINtBhRtjP46IIxzCD18W5reARPfU0oX_APTDHa2LeGCk-oXJlIsjgX-npBxamKfQBrW6_RBONYwbZ5xUCAXw88LGHVkN-FVOZ0OhtiVMoCr4J6Q4Au8s_XoHh8P811OtP7M6ReYSBKDHpJtBZmxGFW_LESMN3I9hwEoSizXqIAQmW0NVffDtJHWlfC2tVdmCLi_tPBTqs5_7B54hT5-GG5HYcOxDub5OG3r4m4SpiFXe3pUSZr27CI_h4DREu6atKBJdjOEPToLgoG6H8_qT6ASl8ylPh6vy5WFZGl9_r-NSDc_UXqBLHkOfLYn_bv6xy1u9N3MMubmsxVVnD0QEMcNUwT444nLLNIMGMHj18w3fO27SgKqn08b5iYU8_6A0iOgG0zdVLR7rYdennoJGTWCNVXekXyRHBJon4IMa-5c2v2RXxMyCTFp7CmurYCMF23YGfUwpNEcSgMkAGT3hDWUcd4wgcv8BPYJVIooRxQbekYto2NDYvw../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93166014 (89M) [application/octet-stream]\n",
            "Saving to: ‘./train.jsonl’\n",
            "\n",
            "./train.jsonl       100%[===================>]  88.85M  11.6MB/s    in 8.3s    \n",
            "\n",
            "2019-11-10 00:17:32 (10.7 MB/s) - ‘./train.jsonl’ saved [93166014/93166014]\n",
            "\n",
            "--2019-11-10 00:17:34--  https://nyu.box.com/shared/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\n",
            "Resolving nyu.box.com (nyu.box.com)... 185.235.236.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl [following]\n",
            "--2019-11-10 00:17:34--  https://nyu.box.com/public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl [following]\n",
            "--2019-11-10 00:17:34--  https://nyu.app.box.com/public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 185.235.236.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!63Xlgz5k-o5nXCCZOUDj7Eg3HF5X6nE2BCmza7be9vY-CcuLwaOPDep8R_YUI8CE7lHMGMEb60M5WXNJ03NfgXsvUnzCzwF5agS6msuf5CgJ-q9g0vBCMhekxVyJujt3pElBKXAJ-d2ppYAMhcyPKz466HNPDipxSvv3fAjGXwxXk7ggSEFJZPWeqddinmb6SpYGQPoONcOalneEct2To3fDKuyG3UzF8pQwNmowgzblCAhW2FiiSNvr779KMKNaGYhuY-6FUTTmj59xaLUUDtAzmtcrCXcQy54O4y7i4ynzjBOh0XlRHKeG4Zfg0_oczWfiT2nGTIgZ9zXhIDL_jHZKoZSiJWke75NNo7NNBXUlmATiHFRkpY1JOeHqxILxs-dmyv8ojTQslkIVTaaiMSimIiozVQmi47mjvvkpn1WBFHID69IwUCYwJI3g21XWBLbfypmW4qcAX0T4Os5t-l2ZWTT-urOFeePf7WoMIRTC_arJpZWiRerbKI8oPTULztEGt5yZo-_9CucTNJ57_LIVigORPF7QCBpFxKeAuxbPA-AtCOfkzNMDsffvM8OO2bLnwWt2bcaY0MMj12hnIsIgxXm_YU95OOQ9lhlzb5LIXL0njhdjxstRpolZVt21zOlmPKSAHaK5vwKJAnqVWdA4Oi5N3Wr8zX0DX2EsgE_tjekDx-FSoEgqnASqSskdSELu5ElGGvotCsA86xN5Hhsj3pUImKCHmGY10SPCZobr-hbDDJ2OVNwkyF3Obm4QwwsCXEFlKDMtxAXPFu9RaWhbM9_fLcmjkQio8nu9-gPkuxvmnlBfYfzpasgqRkYX-CXFoqSrgMvLCoBfv4WVjuFipJop8rNjSwn5hwoyu0LtYABTlJ5W-tOpQMGmc2rKer7TspMdmfE-ufmGzmCsiQxNRUwSN7fZHRLEuyqpPk_P2GZy2NZXCk2oWeoemUBulrovQqA5CGKMhMtpL2A5rqTnPasmppZg_nsO7NvRq07GGt0nBK-dmWQx9QKJihTuxqPw6WVdHpvhMiYCRBibwNPPlFTm75tzpHGXtKZ1ekh5Z7Iic6MEcQ3oRmeFozs9AsRCu7hP51mIPuW1PEO39jmCu6zhCgHqU4rY_PdX9m2e405TPSyfuK_Ka8Nm3xiuk9WuRRI5y9hq6m6UFKpcR2lWJxe4zW6jEH7QVdDNrE3mQwg-3HeP3l-jgoxaIEIeFmpPe4J3n-XX347T06VBVjSTri8eo6gBfwSfFn8RsOQeoIMoAE2oxYYRQglRv8WbksX11yHsL-SUDvgZwlq5pALQhF5E3FbsiG2a_353udbrdq0BxtHlse7B2qknLLgHSFgitkQCDGxFfObXS3nynQiWCry5Rptfzk35Da7lLsnZOTx-YaJ5OXE./download [following]\n",
            "--2019-11-10 00:17:35--  https://public.boxcloud.com/d/1/b1!63Xlgz5k-o5nXCCZOUDj7Eg3HF5X6nE2BCmza7be9vY-CcuLwaOPDep8R_YUI8CE7lHMGMEb60M5WXNJ03NfgXsvUnzCzwF5agS6msuf5CgJ-q9g0vBCMhekxVyJujt3pElBKXAJ-d2ppYAMhcyPKz466HNPDipxSvv3fAjGXwxXk7ggSEFJZPWeqddinmb6SpYGQPoONcOalneEct2To3fDKuyG3UzF8pQwNmowgzblCAhW2FiiSNvr779KMKNaGYhuY-6FUTTmj59xaLUUDtAzmtcrCXcQy54O4y7i4ynzjBOh0XlRHKeG4Zfg0_oczWfiT2nGTIgZ9zXhIDL_jHZKoZSiJWke75NNo7NNBXUlmATiHFRkpY1JOeHqxILxs-dmyv8ojTQslkIVTaaiMSimIiozVQmi47mjvvkpn1WBFHID69IwUCYwJI3g21XWBLbfypmW4qcAX0T4Os5t-l2ZWTT-urOFeePf7WoMIRTC_arJpZWiRerbKI8oPTULztEGt5yZo-_9CucTNJ57_LIVigORPF7QCBpFxKeAuxbPA-AtCOfkzNMDsffvM8OO2bLnwWt2bcaY0MMj12hnIsIgxXm_YU95OOQ9lhlzb5LIXL0njhdjxstRpolZVt21zOlmPKSAHaK5vwKJAnqVWdA4Oi5N3Wr8zX0DX2EsgE_tjekDx-FSoEgqnASqSskdSELu5ElGGvotCsA86xN5Hhsj3pUImKCHmGY10SPCZobr-hbDDJ2OVNwkyF3Obm4QwwsCXEFlKDMtxAXPFu9RaWhbM9_fLcmjkQio8nu9-gPkuxvmnlBfYfzpasgqRkYX-CXFoqSrgMvLCoBfv4WVjuFipJop8rNjSwn5hwoyu0LtYABTlJ5W-tOpQMGmc2rKer7TspMdmfE-ufmGzmCsiQxNRUwSN7fZHRLEuyqpPk_P2GZy2NZXCk2oWeoemUBulrovQqA5CGKMhMtpL2A5rqTnPasmppZg_nsO7NvRq07GGt0nBK-dmWQx9QKJihTuxqPw6WVdHpvhMiYCRBibwNPPlFTm75tzpHGXtKZ1ekh5Z7Iic6MEcQ3oRmeFozs9AsRCu7hP51mIPuW1PEO39jmCu6zhCgHqU4rY_PdX9m2e405TPSyfuK_Ka8Nm3xiuk9WuRRI5y9hq6m6UFKpcR2lWJxe4zW6jEH7QVdDNrE3mQwg-3HeP3l-jgoxaIEIeFmpPe4J3n-XX347T06VBVjSTri8eo6gBfwSfFn8RsOQeoIMoAE2oxYYRQglRv8WbksX11yHsL-SUDvgZwlq5pALQhF5E3FbsiG2a_353udbrdq0BxtHlse7B2qknLLgHSFgitkQCDGxFfObXS3nynQiWCry5Rptfzk35Da7lLsnZOTx-YaJ5OXE./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6044784 (5.8M) [application/octet-stream]\n",
            "Saving to: ‘./valid.jsonl’\n",
            "\n",
            "./valid.jsonl       100%[===================>]   5.76M  5.59MB/s    in 1.0s    \n",
            "\n",
            "2019-11-10 00:17:37 (5.59 MB/s) - ‘./valid.jsonl’ saved [6044784/6044784]\n",
            "\n",
            "--2019-11-10 00:17:38--  https://nyu.box.com/shared/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\n",
            "Resolving nyu.box.com (nyu.box.com)... 185.235.236.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt [following]\n",
            "--2019-11-10 00:17:38--  https://nyu.box.com/public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt [following]\n",
            "--2019-11-10 00:17:39--  https://nyu.app.box.com/public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 185.235.236.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!WmDOxXWRHbvx5b9tff7oX7cFTRetJ7PlEXrrHprydcXOwNu7NE2GspKCjLe-bCsYSk9LS7nBA9DmSSJJrDDafBD74ItoVpysDr_ZM273YJVImKzMes3gjaMdkZ8j24nqxUOWtySGbpeT6cnS7Lg1XS-02OjoJo6Ek7eYIMY3WL8OlWdtQOOp3KKXhthWBcEJhyaES_YiaTMkGX08osS-Lu7V4gkS61zb_-L3fHzDheW9r-lazD_0yPm6ju6fUjhag0nWKoodrNrV-BUNZcglj7MNcKpw6lL5fU1yb06eGeGzrwySz9q2VX8U5rzEtdo5jgzc4tE7nXWMjoYeP0FEEmNKKBvQXmm31vW7xjJk-RJwb5UQNEcI3omK183shKVXp1z4T8k80kmAFOM8wIEBlMtyptFbytgZ1t5Py3albBQTxvGMD5E6YBCPSwBcmQfgWc_BtT6OFR9E8kYTRAfGCMVf0xr5ifemJl7arVg8pRau6bNlZTv5hCzo9Q_6WPJmANBCjEnVyglM9CHjepynpFics2DskdAigr4Eybc0OPcLgPX8npZfhOV40R1-DRlVsV4o8rfz9ShUNDqbop5E5d4qjZU2PZV4JpCc5-Rzchv7kCrVE0BtWfPWlKd-HHXRcYRq58-VqICHFnDPigl82bk-ojpF5rPOHYD4fPfqWvkbusU3LvGXKD_SiN6gRaIIyx8ZB76cz5ONmK1aPIDiuEAEPMJ-LvDePcExgn3ahVpIELG17DpEfWyfq9_uGsjveh3o-9aFKgEeJ0FHOrRkgUilyKwDrAA_uKTrIjlEq25l5BRwF9ETZQU7BLnTw2HkfDKE9OJ6j9t4U96EtxAQykRJDz2fOwKCEw5J2jCNh3AanutgUGzFrbrD8FZGHP8IMQQnZvtEyqIw76Bn1mqDUzZrGtfxqG2cSxJKxcAjgX_CSn7kH7hTp-CFQEPI9p6vq7YVuFSn30xDZH3GoRvY3v-csIRHgACnGMCjeqAYhiIGn45r6EpLG4D4bfFhnviTAYfNSNYpOy7dflPj4BTsghXK1zeUFgH6CxQlQAjnnemqa17etdSkYEPzxmKn3U3aM1MSCb6Vdz-9MeDmf7wVQIn3S3U89p5hGkUAfzuduy6xg6Ie7ef_mYYwxhVckFTWFd7CEV_5KEsJWd4iKTSjuYTGJwS5heNE0ataTFsi1JKRkKr_bifEcuv7gRV9xC4hf4Ti-aRnPRAEZNJNLUJ_wg81gcsTUnKW5NAPx1eG7Y3PGxSTpaSGOIqDi4PF_S2MtPqgcwgx380zGDBbhp_krxN6rcYtkehH1Nvia-8aJaNOqlLvNdm-N3k3A_HMX4nh-iFampK3hNKkekURCmguYF-acmquztWOSutyvHOkHQgjb14vkth4va1jpJ6eLyOQxhUZ-18./download [following]\n",
            "--2019-11-10 00:17:39--  https://public.boxcloud.com/d/1/b1!WmDOxXWRHbvx5b9tff7oX7cFTRetJ7PlEXrrHprydcXOwNu7NE2GspKCjLe-bCsYSk9LS7nBA9DmSSJJrDDafBD74ItoVpysDr_ZM273YJVImKzMes3gjaMdkZ8j24nqxUOWtySGbpeT6cnS7Lg1XS-02OjoJo6Ek7eYIMY3WL8OlWdtQOOp3KKXhthWBcEJhyaES_YiaTMkGX08osS-Lu7V4gkS61zb_-L3fHzDheW9r-lazD_0yPm6ju6fUjhag0nWKoodrNrV-BUNZcglj7MNcKpw6lL5fU1yb06eGeGzrwySz9q2VX8U5rzEtdo5jgzc4tE7nXWMjoYeP0FEEmNKKBvQXmm31vW7xjJk-RJwb5UQNEcI3omK183shKVXp1z4T8k80kmAFOM8wIEBlMtyptFbytgZ1t5Py3albBQTxvGMD5E6YBCPSwBcmQfgWc_BtT6OFR9E8kYTRAfGCMVf0xr5ifemJl7arVg8pRau6bNlZTv5hCzo9Q_6WPJmANBCjEnVyglM9CHjepynpFics2DskdAigr4Eybc0OPcLgPX8npZfhOV40R1-DRlVsV4o8rfz9ShUNDqbop5E5d4qjZU2PZV4JpCc5-Rzchv7kCrVE0BtWfPWlKd-HHXRcYRq58-VqICHFnDPigl82bk-ojpF5rPOHYD4fPfqWvkbusU3LvGXKD_SiN6gRaIIyx8ZB76cz5ONmK1aPIDiuEAEPMJ-LvDePcExgn3ahVpIELG17DpEfWyfq9_uGsjveh3o-9aFKgEeJ0FHOrRkgUilyKwDrAA_uKTrIjlEq25l5BRwF9ETZQU7BLnTw2HkfDKE9OJ6j9t4U96EtxAQykRJDz2fOwKCEw5J2jCNh3AanutgUGzFrbrD8FZGHP8IMQQnZvtEyqIw76Bn1mqDUzZrGtfxqG2cSxJKxcAjgX_CSn7kH7hTp-CFQEPI9p6vq7YVuFSn30xDZH3GoRvY3v-csIRHgACnGMCjeqAYhiIGn45r6EpLG4D4bfFhnviTAYfNSNYpOy7dflPj4BTsghXK1zeUFgH6CxQlQAjnnemqa17etdSkYEPzxmKn3U3aM1MSCb6Vdz-9MeDmf7wVQIn3S3U89p5hGkUAfzuduy6xg6Ie7ef_mYYwxhVckFTWFd7CEV_5KEsJWd4iKTSjuYTGJwS5heNE0ataTFsi1JKRkKr_bifEcuv7gRV9xC4hf4Ti-aRnPRAEZNJNLUJ_wg81gcsTUnKW5NAPx1eG7Y3PGxSTpaSGOIqDi4PF_S2MtPqgcwgx380zGDBbhp_krxN6rcYtkehH1Nvia-8aJaNOqlLvNdm-N3k3A_HMX4nh-iFampK3hNKkekURCmguYF-acmquztWOSutyvHOkHQgjb14vkth4va1jpJ6eLyOQxhUZ-18./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69267718 (66M) [application/octet-stream]\n",
            "Saving to: ‘./chat_model_best_22.pt’\n",
            "\n",
            "./chat_model_best_2 100%[===================>]  66.06M  11.7MB/s    in 6.0s    \n",
            "\n",
            "2019-11-10 00:17:46 (11.0 MB/s) - ‘./chat_model_best_22.pt’ saved [69267718/69267718]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 131438/131438 [00:09<00:00, 13723.67it/s]\n",
            "100%|██████████| 7801/7801 [00:00<00:00, 13683.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybK8QzYNo43z",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5uzfxH4GY_L",
        "colab_type": "code",
        "outputId": "5598c0fe-4dd0-4296-b1ac-aec3adbca0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(train_dataset[0])\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=batchify, batch_size=64)\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=64)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([   7,   10,   12,    5,   21,   14, 3583, 1607,    4,   11,    7,   10,\n",
            "          12,    5,   21,   14,   56,  655,    4,   11,    7,   10,   12,    5,\n",
            "          21,   14, 1806,    8, 2105,    4,   11,    7,   10,   12,   15,   44,\n",
            "        1255,   18,  862,    4,   11,   58,   13,   34,   25,    6,   65,    9,\n",
            "           5,   16,   26,  245,  430,   14,   17,   97, 6377, 2587,   14,  286,\n",
            "          24, 1145,    4]), tensor([  6, 252,  46,  83, 351,   4, 655,  18,  79,  30,  15,  44, 171,   4,\n",
            "          2]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkcvfmC2Hcau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"Encodes the input context.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx=0, dropout=0, shared_lt=None):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "        if shared_lt is None:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.embed_size, pad_idx)\n",
        "        else:\n",
        "            self.embedding = shared_lt\n",
        "            \n",
        "        self.gru = nn.GRU(\n",
        "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "        )\n",
        "        \n",
        "        \n",
        "    def forward(self, text_vec, text_lens, hidden=None, use_packed=True):\n",
        "        embedded = self.embedding(text_vec)\n",
        "        attention_mask = text_vec.ne(self.pad_idx)\n",
        "\n",
        "        embedded = self.dropout(embedded)\n",
        "        if use_packed is True:\n",
        "            embedded = pack_padded_sequence(embedded, text_lens, batch_first=True)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        if use_packed is True:\n",
        "            output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
        "        \n",
        "        return output, hidden, attention_mask\n",
        "\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "        )\n",
        "        \n",
        "        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n",
        "\n",
        "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        self.longest_label = 100\n",
        "\n",
        "    def forward(self, text_vec, decoder_hidden, encoder_states):\n",
        "        emb = self.embedding(text_vec)\n",
        "        emb = self.dropout(emb)\n",
        "        seqlen = text_vec.size(1)\n",
        "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "        \n",
        "        decoder_hidden = decoder_hidden\n",
        "        output = []\n",
        "        attn_w_log = []\n",
        "\n",
        "        for i in range(seqlen):\n",
        "            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n",
        "            \n",
        "            # compute attention at each time step\n",
        "            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n",
        "            output.append(decoder_output_attended)\n",
        "            attn_w_log.append(attn_weights)\n",
        "            \n",
        "        output = torch.cat(output, dim=1).to(text_vec.device)\n",
        "        scores = self.out(output)\n",
        "        \n",
        "        return scores, decoder_hidden, attn_w_log\n",
        "    \n",
        "    def decode_forced(self, ys, encoder_states, xs_lens):\n",
        "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "        \n",
        "        batch_size = ys.size(0)\n",
        "        target_length = ys.size(1)\n",
        "        longest_label = max(target_length, self.longest_label)\n",
        "        \n",
        "        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
        "        \n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
        "        decoder_input = torch.cat([starts, y_in], 1)\n",
        "        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n",
        "        _, preds = decoder_output.max(dim=2)\n",
        "        \n",
        "        return decoder_output, preds, attn_w_log\n",
        "    \n",
        "    \n",
        "class AttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, embedding_size):\n",
        "        super().__init__()\n",
        "        input_dim = hidden_size\n",
        "\n",
        "        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n",
        "\n",
        "        batch_size, seq_length, hidden_size = encoder_output.size()\n",
        "\n",
        "        encoder_output_t = encoder_output.transpose(1,2)\n",
        "        \n",
        "        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n",
        "\n",
        "        attention_scores.masked_fill_((~attention_mask), -10e5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n",
        "\n",
        "        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n",
        "\n",
        "        output = self.linear_out(combined).unsqueeze(1)\n",
        "        output = self.tanh(output)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "    \n",
        "class seq2seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic seq2seq model with attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, opts):\n",
        "\n",
        "        super().__init__()\n",
        "        self.opts = opts\n",
        "        \n",
        "        self.decoder = DecoderRNN(\n",
        "                                    vocab_size=self.opts['vocab_size'],\n",
        "                                    embed_size=self.opts['embedding_size'],\n",
        "                                    hidden_size=self.opts['hidden_size'],\n",
        "                                    num_layers=self.opts['num_layers_dec'],\n",
        "                                    dropout=self.opts['dropout'],\n",
        "                                )\n",
        "        \n",
        "        self.encoder = EncoderRNN(\n",
        "                                    vocab_size=self.opts['vocab_size'],\n",
        "                                    embed_size=self.opts['embedding_size'],\n",
        "                                    hidden_size=self.opts['hidden_size'],\n",
        "                                    num_layers=self.opts['num_layers_enc'],\n",
        "                                    dropout=self.opts['dropout'],\n",
        "                                    shared_lt=self.decoder.embedding\n",
        "        )\n",
        "        \n",
        "    def train(self):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        \n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMsUlw5zHcdM",
        "colab_type": "code",
        "outputId": "6187971f-7be1-4a98-9902-5a644d7ee0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "current_device = 'cuda'\n",
        "model_pt = torch.load('./chat_model_best_22.pt')\n",
        "opts = model_pt['opts']    \n",
        "model = seq2seq(opts)\n",
        "model.load_state_dict(model_pt['state_dict'])\n",
        "model.to(current_device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (decoder): DecoderRNN(\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (embedding): Embedding(18760, 256, padding_idx=0)\n",
              "    (gru): GRU(256, 512, batch_first=True)\n",
              "    (attention): AttentionLayer(\n",
              "      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (softmax): Softmax(dim=-1)\n",
              "      (tanh): Tanh()\n",
              "    )\n",
              "    (out): Linear(in_features=512, out_features=18760, bias=True)\n",
              "  )\n",
              "  (encoder): EncoderRNN(\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (embedding): Embedding(18760, 256, padding_idx=0)\n",
              "    (gru): GRU(256, 512, batch_first=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaT9TBEPHcf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izqfZn_ZHciY",
        "colab_type": "code",
        "outputId": "52c1251d-3c2f-425d-e702-42eb2ba469a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "plot_cache = []            \n",
        "val_loss = 0\n",
        "val_tokens = 0\n",
        "for i, batch in enumerate(valid_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    text_vecs = batch['text_vecs'].to('cuda')\n",
        "    target_vecs = batch['target_vecs'].to('cuda')\n",
        "    \n",
        "    encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
        "    \n",
        "    decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
        "    \n",
        "    scores = decoder_output.view(-1, decoder_output.size(-1))\n",
        "    \n",
        "    loss = criterion(scores, target_vecs.view(-1))\n",
        "    \n",
        "    num_tokens = target_vecs.ne(0).long().sum().item()\n",
        "    \n",
        "    val_tokens += num_tokens\n",
        "    val_loss += loss.item()\n",
        "    break\n",
        "    \n",
        "avg_val_loss = val_loss/val_tokens\n",
        "scheduler.step(avg_val_loss)\n",
        "    \n",
        "print(\"Epoch {} valid loss = {}\".format(0, avg_val_loss))\n",
        "\n",
        "plot_cache.append(avg_val_loss)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 valid loss = 3.7008329438840293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IiebTSGio_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv49FFC5GZL2",
        "colab_type": "code",
        "outputId": "b30097ee-e6f0-4e3f-9cd4-df2160d14277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_IDS = np.random.randint(0,text_vecs.size(0),size=5)\n",
        "BATCH_IDS = BATCH_IDS.tolist()\n",
        "BATCH_IDS"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21, 35, 7, 56, 38]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91XIq0ACTXSJ",
        "colab_type": "code",
        "outputId": "58ab496f-11f0-47e5-af05-424166aceebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attn_w_log[10][BATCH_IDS[0]].shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([243])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6EtIRHYTXUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "attention_list = []\n",
        "for i in range(len(BATCH_IDS)):\n",
        "  batch_id = BATCH_IDS[i]\n",
        "  attentions = torch.zeros((len(attn_w_log),text_vecs.size(1)))\n",
        "  for j in range(len(attn_w_log)):\n",
        "    temp = attn_w_log[j]\n",
        "    attentions[j,:] = temp[batch_id]\n",
        "  attentions = attentions.detach().numpy()\n",
        "  #attentions = attentions.transpose()\n",
        "  attention_list.append(attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZRplPmMTXeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWtMZzUfClop",
        "colab_type": "text"
      },
      "source": [
        "### You present here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou1tB_ldfdiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokens_list = []\n",
        "y_tokens_list = []\n",
        "target_lens = []\n",
        "input_lens = []\n",
        "for batch_id in BATCH_IDS:\n",
        "  target_len = batch[\"target_lens\"][batch_id]\n",
        "  input_len = batch[\"text_lens\"][batch_id]\n",
        "  target_lens.append(target_len)\n",
        "  input_lens.append(input_len)\n",
        "  y_tokens = [chat_dict.ind2word[i] for i in target_vecs[batch_id][:target_len].tolist()]\n",
        "  x_tokens = [chat_dict.ind2word[i] for i in text_vecs[batch_id][:input_len].tolist()]\n",
        "  x_tokens_list.append(x_tokens)\n",
        "  y_tokens_list.append(y_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddl1zGz-Hjhy",
        "colab_type": "code",
        "outputId": "38af8a7b-a81d-4bb5-8157-dc7a73d1a3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attention_list[0][:target_lens[0],:input_lens[0]].shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, 168)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4siV482Xnpcr",
        "colab_type": "code",
        "outputId": "3c5013f1-f1ff-485e-f906-7d0f537ed723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "source": [
        "# Set up figure with colorbar\n",
        "fig = plt.figure(figsize=(150,3))\n",
        "ax1 = fig.add_subplot(151)\n",
        "cax1 = ax1.matshow(attention_list[0][:target_lens[0],:input_lens[0]], cmap='inferno', aspect='auto')\n",
        "fig.colorbar(cax1)\n",
        "\n",
        "# Set up axes\n",
        "ax1.set_xticklabels(['']+ x_tokens_list[0], rotation=0)\n",
        "ax1.set_yticklabels([''] + y_tokens_list[0],rotation=0)\n",
        "\n",
        "# Show label at every tick\n",
        "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax1.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "print(\"Input:\",chat_dict.v2t(text_vecs[BATCH_IDS[0]].tolist()[:input_lens[0]]))\n",
        "print(\"\\n\")\n",
        "print(\"Target:\",chat_dict.v2t(target_vecs[BATCH_IDS[0]].tolist()[:target_lens[0]]))\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: your persona : i have two dogs . \n",
            " your persona : i like to work on vintage cars . \n",
            " your persona : my favorite music is country . \n",
            " your persona : i own two vintage mustangs . \n",
            " hi ! how are you doing tonight ? \n",
            " i ' m doing great . just relaxing with my two dogs . \n",
            " great . in my spare time i do volunteer work . \n",
            " that ' s neat . what kind of volunteer work do you do ? \n",
            " i work in a homeless shelter in my town . \n",
            " good for you . do you like vintage cars ? i ' ve two older mustangs . \n",
            " cool . not really into cars . my day job is wrestling . \n",
            " cars are my thing . vintage cars . i love working on them . wrestling ? do you enjoy it ? \n",
            " yes , i love the crowds , getting to know people .\n",
            "\n",
            "\n",
            "Target: i didn ' t think about the crowd aspect of wrestling . i do not like crowds . __end__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABS0AAADICAYAAAANxMTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 10800x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L09Xc0O7IYiz",
        "colab_type": "code",
        "outputId": "4b7b0ee6-3620-4909-a039-274a171cc75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(y_tokens_list[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fde4W4IqCloq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is some example attention map here, \n",
        "# *make sure you add text tokens on the axes to make it readable!*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KcFY3gVClos",
        "colab_type": "text"
      },
      "source": [
        "![Imgur](https://i.imgur.com/xodciCU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQZBIC3VClot",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Encoder Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFXsGF0oDj8P",
        "colab_type": "code",
        "outputId": "633f7b1c-0afd-4551-afec-b790dd8a3933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 74.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.3)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 76.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 68.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=f90d107816790915c1cb6cc84d3088b337a2c990a3f8b98a302f51dac4a8d284\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, regex, sacremoses, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34OYQS3nEKhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnwwp0lpMxpZ",
        "colab_type": "code",
        "outputId": "fbfb063c-59d0-4bbe-b632-a89368076060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(chat_dict)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18760"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6qfVy-Bplt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class EncoderTransformer(nn.Module):\n",
        "    def __init__(self,vocab_size,max_len,shared_lt,dropout,dim=256, num_layers=2, nhead=4,pad_idx=0):\n",
        "      # you need to add more things here\n",
        "      super().__init__()\n",
        "      self.token_embed = shared_lt\n",
        "      #self.token_embed = nn.Embedding(vocab_size, dim)\n",
        "      self.position_embed = nn.Embedding(max_len, dim)\n",
        "      encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=nhead)\n",
        "      self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "      self.pad_idx = pad_idx\n",
        "      self.dropout = nn.Dropout(p=dropout)\n",
        "      \n",
        "\n",
        "    def forward(self, text_vec):\n",
        "      pos = torch.arange(text_vec.size(1), device=text_vec.device).repeat(1,text_vec.size(0)).view(-1,text_vec.size(1))#.unsqueeze(1)\n",
        "      #print(self.token_embed(text_vec))\n",
        "      #print(self.position_embed(pos))\n",
        "      output = torch.add(self.token_embed(text_vec),self.position_embed(pos))\n",
        "      output = self.dropout(output)\n",
        "      #print(output)\n",
        "      #print(text_vec.shape)\n",
        "      #print(output.shape)\n",
        "      attention_mask = text_vec.eq(self.pad_idx)\n",
        "      \n",
        "      x = self.transformer(output.transpose(0,1),src_key_padding_mask=attention_mask)\n",
        "      #print(output.shape)\n",
        "      x.transpose_(0,1)\n",
        "      #print(x.shape)\n",
        "      hidden = torch.mean(x,dim=1,keepdim=True).transpose(0,1)\n",
        "      return x, hidden , attention_mask\n",
        "\n",
        "\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "        )\n",
        "        \n",
        "        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n",
        "\n",
        "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        self.longest_label = 100\n",
        "\n",
        "    def forward(self, text_vec, decoder_hidden, encoder_states):\n",
        "        emb = self.embedding(text_vec)\n",
        "        emb = self.dropout(emb)\n",
        "        seqlen = text_vec.size(1)\n",
        "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "        \n",
        "        decoder_hidden = decoder_hidden\n",
        "        output = []\n",
        "        attn_w_log = []\n",
        "\n",
        "        for i in range(seqlen):\n",
        "            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n",
        "            \n",
        "            # compute attention at each time step\n",
        "            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n",
        "            output.append(decoder_output_attended)\n",
        "            attn_w_log.append(attn_weights)\n",
        "            \n",
        "        output = torch.cat(output, dim=1).to(text_vec.device)\n",
        "        scores = self.out(output)\n",
        "        \n",
        "        return scores, decoder_hidden, attn_w_log\n",
        "    \n",
        "    def decode_forced(self, ys, encoder_states, xs_lens):\n",
        "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "        \n",
        "        batch_size = ys.size(0)\n",
        "        target_length = ys.size(1)\n",
        "        longest_label = max(target_length, self.longest_label)\n",
        "        \n",
        "        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
        "        \n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
        "        decoder_input = torch.cat([starts, y_in], 1)\n",
        "        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n",
        "        _, preds = decoder_output.max(dim=2)\n",
        "        \n",
        "        return decoder_output, preds, attn_w_log\n",
        "    \n",
        "    \n",
        "class AttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, embedding_size):\n",
        "        super().__init__()\n",
        "        input_dim = hidden_size\n",
        "\n",
        "        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n",
        "\n",
        "        batch_size, seq_length, hidden_size = encoder_output.size()\n",
        "\n",
        "        encoder_output_t = encoder_output.transpose(1,2)\n",
        "        \n",
        "        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n",
        "\n",
        "        attention_scores.masked_fill_((attention_mask), -10e5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n",
        "\n",
        "        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n",
        "\n",
        "        output = self.linear_out(combined).unsqueeze(1)\n",
        "        output = self.tanh(output)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "    \n",
        "class seq2seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic seq2seq model with attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, opts):\n",
        "\n",
        "        super().__init__()\n",
        "        self.opts = opts\n",
        "        \n",
        "        self.decoder = DecoderRNN(\n",
        "                                    vocab_size=self.opts['vocab_size'],\n",
        "                                    embed_size=self.opts['embedding_size'],\n",
        "                                    hidden_size=self.opts['hidden_size'],\n",
        "                                    num_layers=self.opts['num_layers_dec'],\n",
        "                                    dropout=self.opts['dropout'],\n",
        "                                )\n",
        "        \n",
        "        self.encoder = EncoderTransformer(\n",
        "                                    vocab_size=self.opts['vocab_size'],\n",
        "                                    max_len = 10000,shared_lt = self.decoder.embedding,dropout=self.opts[\"dropout\"]\n",
        "                                    #embed_size=self.opts['embedding_size'],\n",
        "                                    #hidden_size=self.opts['hidden_size'],\n",
        "                                    #num_layers=self.opts['num_layers_enc'],\n",
        "                                    #dropout=self.opts['dropout'],\n",
        "                                    #shared_lt=self.decoder.embedding\n",
        "        )\n",
        "        \n",
        "    def train(self):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        \n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-I6KinrHHNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opts = {'dropout': 0.3,\n",
        " 'embedding_size': 256,\n",
        " 'encoder_shared_lt': True,\n",
        " 'hidden_size': 256,\n",
        " 'num_layers_dec': 1,\n",
        " 'num_layers_enc': 2,\n",
        " 'vocab_size': 18760}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGhy3-uGqOr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = seq2seq(opts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SdjLJDQDfcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3, amsgrad=True)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7sCGSKcqkVy",
        "colab_type": "code",
        "outputId": "8126dc8e-ea75-41fa-ecce-4099a648e064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "current_device = \"cuda\"\n",
        "model.to(current_device)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (decoder): DecoderRNN(\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (embedding): Embedding(18760, 256, padding_idx=0)\n",
              "    (gru): GRU(256, 256, batch_first=True)\n",
              "    (attention): AttentionLayer(\n",
              "      (linear_out): Linear(in_features=512, out_features=256, bias=False)\n",
              "      (softmax): Softmax(dim=-1)\n",
              "      (tanh): Tanh()\n",
              "    )\n",
              "    (out): Linear(in_features=256, out_features=18760, bias=True)\n",
              "  )\n",
              "  (encoder): EncoderTransformer(\n",
              "    (token_embed): Embedding(18760, 256, padding_idx=0)\n",
              "    (position_embed): Embedding(10000, 256)\n",
              "    (transformer): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yogkV_jcrHSX",
        "colab_type": "code",
        "outputId": "06a04747-2dbf-4758-ad3a-4dd22c71eb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plot_cache = []\n",
        "\n",
        "best_val_loss = 100\n",
        "\n",
        "for epoch in range(30):\n",
        "    \n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    sum_tokens = 0\n",
        "    \n",
        "    for i, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text_vecs = batch['text_vecs'].to('cuda')\n",
        "        target_vecs = batch['target_vecs'].to('cuda')\n",
        "        \n",
        "        encoded = model.encoder(text_vecs)\n",
        "        \n",
        "        decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
        "        \n",
        "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
        "        \n",
        "        loss = criterion(scores, target_vecs.view(-1))\n",
        "        #print(loss.item())\n",
        "        sum_loss += loss.item()\n",
        "        #print(num_tokens)\n",
        "        num_tokens = target_vecs.ne(0).long().sum().item()\n",
        "        loss /= num_tokens\n",
        "        \n",
        "        sum_tokens += num_tokens\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            avg_train_loss = sum_loss/sum_tokens\n",
        "            print(\"iter {} train loss = {}\".format(i, sum_loss/sum_tokens))\n",
        "    #break        \n",
        "    val_loss = 0\n",
        "    val_tokens = 0\n",
        "    for i, batch in enumerate(valid_loader):\n",
        "        model.eval()\n",
        "        \n",
        "        text_vecs = batch['text_vecs'].to('cuda')\n",
        "        target_vecs = batch['target_vecs'].to('cuda')\n",
        "        \n",
        "        encoded = model.encoder(text_vecs)\n",
        "        \n",
        "        decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
        "        \n",
        "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
        "        \n",
        "        loss = criterion(scores, target_vecs.view(-1))\n",
        "        \n",
        "        num_tokens = target_vecs.ne(0).long().sum().item()\n",
        "        \n",
        "        val_tokens += num_tokens\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    avg_val_loss = val_loss/val_tokens\n",
        "    #scheduler.step(avg_val_loss)\n",
        "        \n",
        "    print(\"Epoch {} valid loss = {}\".format(epoch, avg_val_loss))\n",
        "    \n",
        "    plot_cache.append( (avg_train_loss, avg_val_loss) )\n",
        "    \n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        \n",
        "        torch.save({\n",
        "        'state_dict': model.state_dict(),\n",
        "        'opts': opts,\n",
        "        'plot_cache': plot_cache,\n",
        "            }, f'./transformer_model_best_{epoch}.pt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 train loss = 9.858762548948096\n",
            "iter 100 train loss = 5.795684763699603\n",
            "iter 200 train loss = 5.265336603155197\n",
            "iter 300 train loss = 4.991830030763849\n",
            "iter 400 train loss = 4.820552728722089\n",
            "iter 500 train loss = 4.7003766243487055\n",
            "iter 600 train loss = 4.608378938021887\n",
            "iter 700 train loss = 4.538711303562329\n",
            "iter 800 train loss = 4.478030348250525\n",
            "iter 900 train loss = 4.427710162997196\n",
            "iter 1000 train loss = 4.3867693475210014\n",
            "iter 1100 train loss = 4.352782712638307\n",
            "iter 1200 train loss = 4.3205883344404326\n",
            "iter 1300 train loss = 4.290433093612011\n",
            "iter 1400 train loss = 4.264880636695797\n",
            "iter 1500 train loss = 4.242483382846102\n",
            "iter 1600 train loss = 4.220223540856455\n",
            "iter 1700 train loss = 4.2009459229970085\n",
            "iter 1800 train loss = 4.182403051139307\n",
            "iter 1900 train loss = 4.166725363187934\n",
            "iter 2000 train loss = 4.1518319460409225\n",
            "Epoch 0 valid loss = 3.8969235118970764\n",
            "iter 0 train loss = 3.6174658145203145\n",
            "iter 100 train loss = 3.7344643835883122\n",
            "iter 200 train loss = 3.7362510999597434\n",
            "iter 300 train loss = 3.737510058911677\n",
            "iter 400 train loss = 3.7315824178959143\n",
            "iter 500 train loss = 3.7307260479254105\n",
            "iter 600 train loss = 3.725939309215459\n",
            "iter 700 train loss = 3.7226531696449228\n",
            "iter 800 train loss = 3.7193295890647375\n",
            "iter 900 train loss = 3.716520690326823\n",
            "iter 1000 train loss = 3.715617508139977\n",
            "iter 1100 train loss = 3.713813203701045\n",
            "iter 1200 train loss = 3.7136019718923134\n",
            "iter 1300 train loss = 3.7134820636420494\n",
            "iter 1400 train loss = 3.713154876146608\n",
            "iter 1500 train loss = 3.7122748876431704\n",
            "iter 1600 train loss = 3.709238641033615\n",
            "iter 1700 train loss = 3.707482557702869\n",
            "iter 1800 train loss = 3.7073317449249\n",
            "iter 1900 train loss = 3.706658373777705\n",
            "iter 2000 train loss = 3.704703407132038\n",
            "Epoch 1 valid loss = 3.7885231202586453\n",
            "iter 0 train loss = 3.5280057916230563\n",
            "iter 100 train loss = 3.5407452682833944\n",
            "iter 200 train loss = 3.561321874060009\n",
            "iter 300 train loss = 3.563290103305536\n",
            "iter 400 train loss = 3.5628968157890144\n",
            "iter 500 train loss = 3.5674944543516003\n",
            "iter 600 train loss = 3.5635115913252475\n",
            "iter 700 train loss = 3.5675422306533795\n",
            "iter 800 train loss = 3.566378530316322\n",
            "iter 900 train loss = 3.566688539229226\n",
            "iter 1000 train loss = 3.5680852737537316\n",
            "iter 1100 train loss = 3.570207772768334\n",
            "iter 1200 train loss = 3.5703034315874307\n",
            "iter 1300 train loss = 3.5702984502876793\n",
            "iter 1400 train loss = 3.57135047305909\n",
            "iter 1500 train loss = 3.5706810423456212\n",
            "iter 1600 train loss = 3.5726822120555264\n",
            "iter 1700 train loss = 3.5724990451577825\n",
            "iter 1800 train loss = 3.57226637720908\n",
            "iter 1900 train loss = 3.5720295701389384\n",
            "iter 2000 train loss = 3.571975477451445\n",
            "Epoch 2 valid loss = 3.7631087030954227\n",
            "iter 0 train loss = 3.185059768915463\n",
            "iter 100 train loss = 3.4671987358555225\n",
            "iter 200 train loss = 3.4602704084187454\n",
            "iter 300 train loss = 3.4629237268650694\n",
            "iter 400 train loss = 3.4657101480500687\n",
            "iter 500 train loss = 3.468276130896295\n",
            "iter 600 train loss = 3.4694996271408125\n",
            "iter 700 train loss = 3.47240713030699\n",
            "iter 800 train loss = 3.4760056986212358\n",
            "iter 900 train loss = 3.4772867508408822\n",
            "iter 1000 train loss = 3.4775609711167688\n",
            "iter 1100 train loss = 3.4794318527916688\n",
            "iter 1200 train loss = 3.4810122137267765\n",
            "iter 1300 train loss = 3.4817736120208127\n",
            "iter 1400 train loss = 3.4837884486242974\n",
            "iter 1500 train loss = 3.483320591143565\n",
            "iter 1600 train loss = 3.4840103587573985\n",
            "iter 1700 train loss = 3.4854059564748043\n",
            "iter 1800 train loss = 3.486020386220789\n",
            "iter 1900 train loss = 3.486832598829998\n",
            "iter 2000 train loss = 3.4873130378207997\n",
            "Epoch 3 valid loss = 3.7548240640141404\n",
            "iter 0 train loss = 3.3683359476857886\n",
            "iter 100 train loss = 3.3877427934116824\n",
            "iter 200 train loss = 3.3989117191619895\n",
            "iter 300 train loss = 3.3997739475934208\n",
            "iter 400 train loss = 3.403308967874155\n",
            "iter 500 train loss = 3.40201897902592\n",
            "iter 600 train loss = 3.405998065373748\n",
            "iter 700 train loss = 3.407699428713185\n",
            "iter 800 train loss = 3.4112826070307354\n",
            "iter 900 train loss = 3.414102111615627\n",
            "iter 1000 train loss = 3.4149160045038\n",
            "iter 1100 train loss = 3.416029593373157\n",
            "iter 1200 train loss = 3.418598615311203\n",
            "iter 1300 train loss = 3.4201605086679643\n",
            "iter 1400 train loss = 3.4214247538380587\n",
            "iter 1500 train loss = 3.422809606352192\n",
            "iter 1600 train loss = 3.422951762697038\n",
            "iter 1700 train loss = 3.4256195039973516\n",
            "iter 1800 train loss = 3.4273350073757944\n",
            "iter 1900 train loss = 3.4284938574987858\n",
            "iter 2000 train loss = 3.429058544805124\n",
            "Epoch 4 valid loss = 3.7498683900669376\n",
            "iter 0 train loss = 3.2274884514870434\n",
            "iter 100 train loss = 3.3140502723613556\n",
            "iter 200 train loss = 3.33400985917532\n",
            "iter 300 train loss = 3.337486719318141\n",
            "iter 400 train loss = 3.3469745311687316\n",
            "iter 500 train loss = 3.3524514338962472\n",
            "iter 600 train loss = 3.357890637294321\n",
            "iter 700 train loss = 3.3592578180949366\n",
            "iter 800 train loss = 3.3630973359718803\n",
            "iter 900 train loss = 3.3651431256584554\n",
            "iter 1000 train loss = 3.367734563157392\n",
            "iter 1100 train loss = 3.369031132619748\n",
            "iter 1200 train loss = 3.3710974068773525\n",
            "iter 1300 train loss = 3.3735664280497306\n",
            "iter 1400 train loss = 3.3751780025665377\n",
            "iter 1500 train loss = 3.3760406764052684\n",
            "iter 1600 train loss = 3.377969773404248\n",
            "iter 1700 train loss = 3.3802773601090577\n",
            "iter 1800 train loss = 3.3796165218719914\n",
            "iter 1900 train loss = 3.3805573143246974\n",
            "iter 2000 train loss = 3.3811481829548007\n",
            "Epoch 5 valid loss = 3.749859360680599\n",
            "iter 0 train loss = 3.3709792035180817\n",
            "iter 100 train loss = 3.304722231905092\n",
            "iter 200 train loss = 3.312238684406987\n",
            "iter 300 train loss = 3.308494961802933\n",
            "iter 400 train loss = 3.3123834942219625\n",
            "iter 500 train loss = 3.3161733388326198\n",
            "iter 600 train loss = 3.3189490285967786\n",
            "iter 700 train loss = 3.320771632853761\n",
            "iter 800 train loss = 3.3213547654213778\n",
            "iter 900 train loss = 3.3265133162922584\n",
            "iter 1000 train loss = 3.3277137613366716\n",
            "iter 1100 train loss = 3.3311745800532395\n",
            "iter 1200 train loss = 3.333207855832933\n",
            "iter 1300 train loss = 3.333419601838779\n",
            "iter 1400 train loss = 3.3366085260421308\n",
            "iter 1500 train loss = 3.3380251771364615\n",
            "iter 1600 train loss = 3.339104156152923\n",
            "iter 1700 train loss = 3.340835029280129\n",
            "iter 1800 train loss = 3.3414548968565208\n",
            "iter 1900 train loss = 3.3433472859050313\n",
            "iter 2000 train loss = 3.3442103206851024\n",
            "Epoch 6 valid loss = 3.7616114577127635\n",
            "iter 0 train loss = 3.330734198498811\n",
            "iter 100 train loss = 3.286484024321819\n",
            "iter 200 train loss = 3.2919174795989603\n",
            "iter 300 train loss = 3.291813154770922\n",
            "iter 400 train loss = 3.289726880497209\n",
            "iter 500 train loss = 3.291279070865309\n",
            "iter 600 train loss = 3.292668269647003\n",
            "iter 700 train loss = 3.2975080129377643\n",
            "iter 800 train loss = 3.297485552926715\n",
            "iter 900 train loss = 3.2998987369965076\n",
            "iter 1000 train loss = 3.3027059257467033\n",
            "iter 1100 train loss = 3.3032079122565463\n",
            "iter 1200 train loss = 3.304297149820456\n",
            "iter 1300 train loss = 3.3041608701901644\n",
            "iter 1400 train loss = 3.3043749937371505\n",
            "iter 1500 train loss = 3.30593720452925\n",
            "iter 1600 train loss = 3.3077968652703436\n",
            "iter 1700 train loss = 3.3085302204287905\n",
            "iter 1800 train loss = 3.309955947936524\n",
            "iter 1900 train loss = 3.3115048630897923\n",
            "iter 2000 train loss = 3.312737356961983\n",
            "Epoch 7 valid loss = 3.7621562893361262\n",
            "iter 0 train loss = 3.1561740590857967\n",
            "iter 100 train loss = 3.2477971553762366\n",
            "iter 200 train loss = 3.2474689151814675\n",
            "iter 300 train loss = 3.248971317005016\n",
            "iter 400 train loss = 3.252238719308788\n",
            "iter 500 train loss = 3.2571393947102725\n",
            "iter 600 train loss = 3.259198110383637\n",
            "iter 700 train loss = 3.261271588048345\n",
            "iter 800 train loss = 3.2624137463011085\n",
            "iter 900 train loss = 3.264911069623123\n",
            "iter 1000 train loss = 3.2663023496923125\n",
            "iter 1100 train loss = 3.269745842406534\n",
            "iter 1200 train loss = 3.2715893187611584\n",
            "iter 1300 train loss = 3.2719978813637134\n",
            "iter 1400 train loss = 3.273555941406186\n",
            "iter 1500 train loss = 3.275873955139795\n",
            "iter 1600 train loss = 3.2774054231060767\n",
            "iter 1700 train loss = 3.278975729101753\n",
            "iter 1800 train loss = 3.2818647128300897\n",
            "iter 1900 train loss = 3.2837348072431376\n",
            "iter 2000 train loss = 3.2852618618532805\n",
            "Epoch 8 valid loss = 3.7695315666971045\n",
            "iter 0 train loss = 3.1526202733867037\n",
            "iter 100 train loss = 3.193824765237155\n",
            "iter 200 train loss = 3.206170448930781\n",
            "iter 300 train loss = 3.217210412302868\n",
            "iter 400 train loss = 3.222932745980534\n",
            "iter 500 train loss = 3.224055335155093\n",
            "iter 600 train loss = 3.229019736822229\n",
            "iter 700 train loss = 3.232414013323386\n",
            "iter 800 train loss = 3.236028987047176\n",
            "iter 900 train loss = 3.2390008503395102\n",
            "iter 1000 train loss = 3.242145321703827\n",
            "iter 1100 train loss = 3.2454547787065984\n",
            "iter 1200 train loss = 3.248178627080751\n",
            "iter 1300 train loss = 3.2504929985223825\n",
            "iter 1400 train loss = 3.2522080045422785\n",
            "iter 1500 train loss = 3.253402081788841\n",
            "iter 1600 train loss = 3.254345090329354\n",
            "iter 1700 train loss = 3.255433368796022\n",
            "iter 1800 train loss = 3.2579487386388073\n",
            "iter 1900 train loss = 3.260535094115418\n",
            "iter 2000 train loss = 3.262376638644463\n",
            "Epoch 9 valid loss = 3.773349122740575\n",
            "iter 0 train loss = 2.970947265625\n",
            "iter 100 train loss = 3.1795136438884066\n",
            "iter 200 train loss = 3.1910944141968307\n",
            "iter 300 train loss = 3.1958414751171182\n",
            "iter 400 train loss = 3.1994858413757132\n",
            "iter 500 train loss = 3.201688047987064\n",
            "iter 600 train loss = 3.208368130208522\n",
            "iter 700 train loss = 3.2111289220407864\n",
            "iter 800 train loss = 3.213038998231984\n",
            "iter 900 train loss = 3.2151071817632215\n",
            "iter 1000 train loss = 3.218031401876255\n",
            "iter 1100 train loss = 3.221121053931498\n",
            "iter 1200 train loss = 3.224743954202701\n",
            "iter 1300 train loss = 3.2270636967030653\n",
            "iter 1400 train loss = 3.2289149482317434\n",
            "iter 1500 train loss = 3.230531891786636\n",
            "iter 1600 train loss = 3.233875033865506\n",
            "iter 1700 train loss = 3.236736654653509\n",
            "iter 1800 train loss = 3.2383344000912007\n",
            "iter 1900 train loss = 3.2393489722981768\n",
            "iter 2000 train loss = 3.240555290503076\n",
            "Epoch 10 valid loss = 3.788362458644519\n",
            "iter 0 train loss = 2.9848750652653155\n",
            "iter 100 train loss = 3.175515234337567\n",
            "iter 200 train loss = 3.1831233374412222\n",
            "iter 300 train loss = 3.1907547071849005\n",
            "iter 400 train loss = 3.1953102585369813\n",
            "iter 500 train loss = 3.1991358219189947\n",
            "iter 600 train loss = 3.2023258164702093\n",
            "iter 700 train loss = 3.205248907909214\n",
            "iter 800 train loss = 3.2089148734341073\n",
            "iter 900 train loss = 3.211839884542065\n",
            "iter 1000 train loss = 3.212675451480023\n",
            "iter 1100 train loss = 3.2146779833729076\n",
            "iter 1200 train loss = 3.21527435927706\n",
            "iter 1300 train loss = 3.2161982066243042\n",
            "iter 1400 train loss = 3.2180036342615965\n",
            "iter 1500 train loss = 3.218272260866998\n",
            "iter 1600 train loss = 3.22044354696397\n",
            "iter 1700 train loss = 3.2222389139811707\n",
            "iter 1800 train loss = 3.2214149913282575\n",
            "iter 1900 train loss = 3.223174324703831\n",
            "iter 2000 train loss = 3.223961030375558\n",
            "Epoch 11 valid loss = 3.786910920864722\n",
            "iter 0 train loss = 3.207475895255939\n",
            "iter 100 train loss = 3.1689666350686396\n",
            "iter 200 train loss = 3.1620319430607378\n",
            "iter 300 train loss = 3.1617210649482366\n",
            "iter 400 train loss = 3.1685899574281633\n",
            "iter 500 train loss = 3.1704339926013234\n",
            "iter 600 train loss = 3.1749479551884052\n",
            "iter 700 train loss = 3.1774727722865954\n",
            "iter 800 train loss = 3.181125914856047\n",
            "iter 900 train loss = 3.1824277945432162\n",
            "iter 1000 train loss = 3.184966259647665\n",
            "iter 1100 train loss = 3.1896759384337887\n",
            "iter 1200 train loss = 3.190432570606775\n",
            "iter 1300 train loss = 3.1922122748538415\n",
            "iter 1400 train loss = 3.1940798056499218\n",
            "iter 1500 train loss = 3.1973154771117667\n",
            "iter 1600 train loss = 3.1988395591134564\n",
            "iter 1700 train loss = 3.201492526464807\n",
            "iter 1800 train loss = 3.202255197310415\n",
            "iter 1900 train loss = 3.2045482313793636\n",
            "iter 2000 train loss = 3.2062829811071145\n",
            "Epoch 12 valid loss = 3.7936632174864395\n",
            "iter 0 train loss = 3.353583757267442\n",
            "iter 100 train loss = 3.1300363604323307\n",
            "iter 200 train loss = 3.141563592392733\n",
            "iter 300 train loss = 3.1389117891048204\n",
            "iter 400 train loss = 3.145237525225406\n",
            "iter 500 train loss = 3.1498058620353593\n",
            "iter 600 train loss = 3.154822091235745\n",
            "iter 700 train loss = 3.1602501462789725\n",
            "iter 800 train loss = 3.1629946162547156\n",
            "iter 900 train loss = 3.166151247809827\n",
            "iter 1000 train loss = 3.1677908012270084\n",
            "iter 1100 train loss = 3.170088100084389\n",
            "iter 1200 train loss = 3.173939433987844\n",
            "iter 1300 train loss = 3.1768913586165226\n",
            "iter 1400 train loss = 3.178825283204435\n",
            "iter 1500 train loss = 3.181295247981132\n",
            "iter 1600 train loss = 3.1828713567496982\n",
            "iter 1700 train loss = 3.1852069543106234\n",
            "iter 1800 train loss = 3.1862724009972045\n",
            "iter 1900 train loss = 3.187466748722099\n",
            "iter 2000 train loss = 3.1888004217224806\n",
            "Epoch 13 valid loss = 3.806155416884867\n",
            "iter 0 train loss = 3.0646516730986444\n",
            "iter 100 train loss = 3.118909571514172\n",
            "iter 200 train loss = 3.1231426557306707\n",
            "iter 300 train loss = 3.1282485518295746\n",
            "iter 400 train loss = 3.1321468068392857\n",
            "iter 500 train loss = 3.134729039263777\n",
            "iter 600 train loss = 3.1424982775123427\n",
            "iter 700 train loss = 3.146475975459649\n",
            "iter 800 train loss = 3.1504109757098755\n",
            "iter 900 train loss = 3.1513549990941647\n",
            "iter 1000 train loss = 3.154583823721993\n",
            "iter 1100 train loss = 3.15829065586272\n",
            "iter 1200 train loss = 3.16013037351967\n",
            "iter 1300 train loss = 3.161515246613328\n",
            "iter 1400 train loss = 3.163548566622595\n",
            "iter 1500 train loss = 3.1663033796324824\n",
            "iter 1600 train loss = 3.167824433037893\n",
            "iter 1700 train loss = 3.1694374580741957\n",
            "iter 1800 train loss = 3.1716257633502667\n",
            "iter 1900 train loss = 3.1740385385062546\n",
            "iter 2000 train loss = 3.1755363633277836\n",
            "Epoch 14 valid loss = 3.8081391911268807\n",
            "iter 0 train loss = 3.025700266768293\n",
            "iter 100 train loss = 3.1298062382669927\n",
            "iter 200 train loss = 3.1286776991359964\n",
            "iter 300 train loss = 3.13221662395216\n",
            "iter 400 train loss = 3.133644820264319\n",
            "iter 500 train loss = 3.1354357996576354\n",
            "iter 600 train loss = 3.1364453478017973\n",
            "iter 700 train loss = 3.1408366927519276\n",
            "iter 800 train loss = 3.143227017392038\n",
            "iter 900 train loss = 3.1454306301422914\n",
            "iter 1000 train loss = 3.1482004225060414\n",
            "iter 1100 train loss = 3.150955574761147\n",
            "iter 1200 train loss = 3.1533473456435974\n",
            "iter 1300 train loss = 3.1538873352745105\n",
            "iter 1400 train loss = 3.1559201232678378\n",
            "iter 1500 train loss = 3.156794767966743\n",
            "iter 1600 train loss = 3.159042685642906\n",
            "iter 1700 train loss = 3.1597182383453224\n",
            "iter 1800 train loss = 3.1603805683714303\n",
            "iter 1900 train loss = 3.162044907423786\n",
            "iter 2000 train loss = 3.1642133562867776\n",
            "Epoch 15 valid loss = 3.8187361882527915\n",
            "iter 0 train loss = 2.9883096732345282\n",
            "iter 100 train loss = 3.082353437089885\n",
            "iter 200 train loss = 3.0955187613966912\n",
            "iter 300 train loss = 3.1070698743379586\n",
            "iter 400 train loss = 3.1179257973573336\n",
            "iter 500 train loss = 3.120839412581437\n",
            "iter 600 train loss = 3.126257990749625\n",
            "iter 700 train loss = 3.1242652816591434\n",
            "iter 800 train loss = 3.125272895542068\n",
            "iter 900 train loss = 3.126696177793809\n",
            "iter 1000 train loss = 3.1290403164678264\n",
            "iter 1100 train loss = 3.130345498704487\n",
            "iter 1200 train loss = 3.13500235871339\n",
            "iter 1300 train loss = 3.1368911272485698\n",
            "iter 1400 train loss = 3.1386443112263045\n",
            "iter 1500 train loss = 3.14127522563582\n",
            "iter 1600 train loss = 3.144367150712954\n",
            "iter 1700 train loss = 3.1466825521058635\n",
            "iter 1800 train loss = 3.1486040356661498\n",
            "iter 1900 train loss = 3.1509033915267235\n",
            "iter 2000 train loss = 3.1519889575402753\n",
            "Epoch 16 valid loss = 3.8296336206734765\n",
            "iter 0 train loss = 2.9984170713044134\n",
            "iter 100 train loss = 3.089214176581692\n",
            "iter 200 train loss = 3.094781415344289\n",
            "iter 300 train loss = 3.0983885017401045\n",
            "iter 400 train loss = 3.10478771369889\n",
            "iter 500 train loss = 3.10599183857792\n",
            "iter 600 train loss = 3.1084408092166114\n",
            "iter 700 train loss = 3.112410276063318\n",
            "iter 800 train loss = 3.115407030861764\n",
            "iter 900 train loss = 3.117677941200327\n",
            "iter 1000 train loss = 3.118602308317053\n",
            "iter 1100 train loss = 3.1211638234716617\n",
            "iter 1200 train loss = 3.1240138754719173\n",
            "iter 1300 train loss = 3.126610396884015\n",
            "iter 1400 train loss = 3.129937478675876\n",
            "iter 1500 train loss = 3.132001906585225\n",
            "iter 1600 train loss = 3.13405978883712\n",
            "iter 1700 train loss = 3.135123338262016\n",
            "iter 1800 train loss = 3.137227958321078\n",
            "iter 1900 train loss = 3.138849730751572\n",
            "iter 2000 train loss = 3.1408676729363285\n",
            "Epoch 17 valid loss = 3.8315942079956233\n",
            "iter 0 train loss = 3.216747033142301\n",
            "iter 100 train loss = 3.080140285275329\n",
            "iter 200 train loss = 3.0862006977767056\n",
            "iter 300 train loss = 3.0913279253767376\n",
            "iter 400 train loss = 3.093518907600688\n",
            "iter 500 train loss = 3.0979519860009663\n",
            "iter 600 train loss = 3.0994450475446462\n",
            "iter 700 train loss = 3.1034353343188434\n",
            "iter 800 train loss = 3.1083133648034527\n",
            "iter 900 train loss = 3.1105716763766913\n",
            "iter 1000 train loss = 3.113038388755292\n",
            "iter 1100 train loss = 3.1159193873172306\n",
            "iter 1200 train loss = 3.118369016523549\n",
            "iter 1300 train loss = 3.119530989954122\n",
            "iter 1400 train loss = 3.12072678910012\n",
            "iter 1500 train loss = 3.121626390079955\n",
            "iter 1600 train loss = 3.1227573151612438\n",
            "iter 1700 train loss = 3.123980499372605\n",
            "iter 1800 train loss = 3.125553149038009\n",
            "iter 1900 train loss = 3.12879187737398\n",
            "iter 2000 train loss = 3.1308264482978645\n",
            "Epoch 18 valid loss = 3.841688986853026\n",
            "iter 0 train loss = 2.907496121364878\n",
            "iter 100 train loss = 3.0579928858206795\n",
            "iter 200 train loss = 3.069552298091046\n",
            "iter 300 train loss = 3.0787375184415406\n",
            "iter 400 train loss = 3.084009160949708\n",
            "iter 500 train loss = 3.082735784573123\n",
            "iter 600 train loss = 3.085387726802054\n",
            "iter 700 train loss = 3.0918778887323275\n",
            "iter 800 train loss = 3.0951416020430984\n",
            "iter 900 train loss = 3.0983709992911486\n",
            "iter 1000 train loss = 3.100635931360009\n",
            "iter 1100 train loss = 3.1039409764962143\n",
            "iter 1200 train loss = 3.1071374918585324\n",
            "iter 1300 train loss = 3.109383701367524\n",
            "iter 1400 train loss = 3.1113873316678435\n",
            "iter 1500 train loss = 3.11252553139105\n",
            "iter 1600 train loss = 3.114754360676587\n",
            "iter 1700 train loss = 3.1172724108187735\n",
            "iter 1800 train loss = 3.117857002702458\n",
            "iter 1900 train loss = 3.1187922525152536\n",
            "iter 2000 train loss = 3.1209724569371\n",
            "Epoch 19 valid loss = 3.8410818480067093\n",
            "iter 0 train loss = 3.1307257555202095\n",
            "iter 100 train loss = 3.057438281042627\n",
            "iter 200 train loss = 3.0694717564013243\n",
            "iter 300 train loss = 3.075269933760477\n",
            "iter 400 train loss = 3.076737959428032\n",
            "iter 500 train loss = 3.077767690478549\n",
            "iter 600 train loss = 3.0801713899661456\n",
            "iter 700 train loss = 3.0855745491306146\n",
            "iter 800 train loss = 3.0916495646485176\n",
            "iter 900 train loss = 3.0937838361802847\n",
            "iter 1000 train loss = 3.0958800046119945\n",
            "iter 1100 train loss = 3.099091114238983\n",
            "iter 1200 train loss = 3.1008037000820914\n",
            "iter 1300 train loss = 3.1017348189579925\n",
            "iter 1400 train loss = 3.1045136082225384\n",
            "iter 1500 train loss = 3.1053632369212503\n",
            "iter 1600 train loss = 3.1067153153964475\n",
            "iter 1700 train loss = 3.1070833640101676\n",
            "iter 1800 train loss = 3.109787984749927\n",
            "iter 1900 train loss = 3.11095697482918\n",
            "iter 2000 train loss = 3.112607357382068\n",
            "Epoch 20 valid loss = 3.849619338056321\n",
            "iter 0 train loss = 3.092155045377994\n",
            "iter 100 train loss = 3.047928306483572\n",
            "iter 200 train loss = 3.0627763509740293\n",
            "iter 300 train loss = 3.0675824019879316\n",
            "iter 400 train loss = 3.073021821483532\n",
            "iter 500 train loss = 3.0712206655434087\n",
            "iter 600 train loss = 3.0759938156129656\n",
            "iter 700 train loss = 3.0792158490103927\n",
            "iter 800 train loss = 3.0806023068883843\n",
            "iter 900 train loss = 3.083540707192553\n",
            "iter 1000 train loss = 3.084380559163148\n",
            "iter 1100 train loss = 3.0851955495417984\n",
            "iter 1200 train loss = 3.088621817028156\n",
            "iter 1300 train loss = 3.0917561976801426\n",
            "iter 1400 train loss = 3.0946441830083073\n",
            "iter 1500 train loss = 3.0974032511418392\n",
            "iter 1600 train loss = 3.0986289385924346\n",
            "iter 1700 train loss = 3.1002002550247285\n",
            "iter 1800 train loss = 3.101424421259266\n",
            "iter 1900 train loss = 3.1030192028497185\n",
            "iter 2000 train loss = 3.10454061773511\n",
            "Epoch 21 valid loss = 3.8569243086600733\n",
            "iter 0 train loss = 2.9251112331475997\n",
            "iter 100 train loss = 3.034742711485668\n",
            "iter 200 train loss = 3.0554187059353497\n",
            "iter 300 train loss = 3.0575174490521193\n",
            "iter 400 train loss = 3.063757038474915\n",
            "iter 500 train loss = 3.0623978743121754\n",
            "iter 600 train loss = 3.0657674407477358\n",
            "iter 700 train loss = 3.071019416329626\n",
            "iter 800 train loss = 3.074964938973059\n",
            "iter 900 train loss = 3.0761058840300133\n",
            "iter 1000 train loss = 3.0772987726059253\n",
            "iter 1100 train loss = 3.0804408982757554\n",
            "iter 1200 train loss = 3.0818836449411537\n",
            "iter 1300 train loss = 3.083486698826689\n",
            "iter 1400 train loss = 3.0859298062746756\n",
            "iter 1500 train loss = 3.088465628866399\n",
            "iter 1600 train loss = 3.090195238416779\n",
            "iter 1700 train loss = 3.092579871241634\n",
            "iter 1800 train loss = 3.0941513450914506\n",
            "iter 1900 train loss = 3.096080140367404\n",
            "iter 2000 train loss = 3.0975830855280932\n",
            "Epoch 22 valid loss = 3.8586285496162174\n",
            "iter 0 train loss = 3.224183930416175\n",
            "iter 100 train loss = 3.0310062595627474\n",
            "iter 200 train loss = 3.0386220223548652\n",
            "iter 300 train loss = 3.0451432803453105\n",
            "iter 400 train loss = 3.0506503356165386\n",
            "iter 500 train loss = 3.0536517702033934\n",
            "iter 600 train loss = 3.057247712505711\n",
            "iter 700 train loss = 3.0598369905710654\n",
            "iter 800 train loss = 3.0627722735768974\n",
            "iter 900 train loss = 3.0650715147882805\n",
            "iter 1000 train loss = 3.0670960470719866\n",
            "iter 1100 train loss = 3.0715767590269447\n",
            "iter 1200 train loss = 3.0726225819947284\n",
            "iter 1300 train loss = 3.075769936592928\n",
            "iter 1400 train loss = 3.0777401334835246\n",
            "iter 1500 train loss = 3.0801648615907986\n",
            "iter 1600 train loss = 3.082193800259104\n",
            "iter 1700 train loss = 3.084250978873212\n",
            "iter 1800 train loss = 3.085824916354282\n",
            "iter 1900 train loss = 3.0880156006859245\n",
            "iter 2000 train loss = 3.0895617296432696\n",
            "Epoch 23 valid loss = 3.8681022361874646\n",
            "iter 0 train loss = 3.103664029909118\n",
            "iter 100 train loss = 3.016210027695502\n",
            "iter 200 train loss = 3.0239793296920214\n",
            "iter 300 train loss = 3.031540493395242\n",
            "iter 400 train loss = 3.0442113063986493\n",
            "iter 500 train loss = 3.0459021775276\n",
            "iter 600 train loss = 3.052116367688716\n",
            "iter 700 train loss = 3.057292212124556\n",
            "iter 800 train loss = 3.0596023960785814\n",
            "iter 900 train loss = 3.0613025052294214\n",
            "iter 1000 train loss = 3.0620479058764114\n",
            "iter 1100 train loss = 3.064598296808319\n",
            "iter 1200 train loss = 3.0659150497273453\n",
            "iter 1300 train loss = 3.0682032312105947\n",
            "iter 1400 train loss = 3.0712698782420995\n",
            "iter 1500 train loss = 3.074964424548062\n",
            "iter 1600 train loss = 3.077457884834309\n",
            "iter 1700 train loss = 3.0794732832225415\n",
            "iter 1800 train loss = 3.0811344978776165\n",
            "iter 1900 train loss = 3.082273704532271\n",
            "iter 2000 train loss = 3.0837906677305376\n",
            "Epoch 24 valid loss = 3.8717011046695284\n",
            "iter 0 train loss = 3.013595473345588\n",
            "iter 100 train loss = 3.040925642358102\n",
            "iter 200 train loss = 3.0407480855420017\n",
            "iter 300 train loss = 3.0405232867881846\n",
            "iter 400 train loss = 3.042804395418817\n",
            "iter 500 train loss = 3.0465767173546747\n",
            "iter 600 train loss = 3.0517864760289473\n",
            "iter 700 train loss = 3.0523290691695166\n",
            "iter 800 train loss = 3.051870943406552\n",
            "iter 900 train loss = 3.0534253241005644\n",
            "iter 1000 train loss = 3.0589991574446223\n",
            "iter 1100 train loss = 3.0613367703637335\n",
            "iter 1200 train loss = 3.063292909763863\n",
            "iter 1300 train loss = 3.0651814178634993\n",
            "iter 1400 train loss = 3.067681291389095\n",
            "iter 1500 train loss = 3.06897758364695\n",
            "iter 1600 train loss = 3.0694129654283997\n",
            "iter 1700 train loss = 3.071536336854872\n",
            "iter 1800 train loss = 3.0724581053469264\n",
            "iter 1900 train loss = 3.074648323833389\n",
            "iter 2000 train loss = 3.076648006440491\n",
            "Epoch 25 valid loss = 3.882451866450398\n",
            "iter 0 train loss = 3.1582330075734393\n",
            "iter 100 train loss = 3.0347750233169886\n",
            "iter 200 train loss = 3.0287593424586543\n",
            "iter 300 train loss = 3.0317846884776873\n",
            "iter 400 train loss = 3.038791360297579\n",
            "iter 500 train loss = 3.041642348003062\n",
            "iter 600 train loss = 3.0450184453179245\n",
            "iter 700 train loss = 3.0487199808047927\n",
            "iter 800 train loss = 3.048604844960427\n",
            "iter 900 train loss = 3.0497124303304886\n",
            "iter 1000 train loss = 3.052954611839857\n",
            "iter 1100 train loss = 3.0540479383646453\n",
            "iter 1200 train loss = 3.0554092799165318\n",
            "iter 1300 train loss = 3.0587676591482724\n",
            "iter 1400 train loss = 3.060262922265294\n",
            "iter 1500 train loss = 3.0622372046298634\n",
            "iter 1600 train loss = 3.063385730966389\n",
            "iter 1700 train loss = 3.065570836396298\n",
            "iter 1800 train loss = 3.0674064610431566\n",
            "iter 1900 train loss = 3.06964421161028\n",
            "iter 2000 train loss = 3.0705849092601523\n",
            "Epoch 26 valid loss = 3.8872717453709487\n",
            "iter 0 train loss = 3.1163063321147564\n",
            "iter 100 train loss = 3.0215675183404773\n",
            "iter 200 train loss = 3.0182805663473142\n",
            "iter 300 train loss = 3.0215585109922642\n",
            "iter 400 train loss = 3.0271715810664803\n",
            "iter 500 train loss = 3.0309112591762606\n",
            "iter 600 train loss = 3.031934618789256\n",
            "iter 700 train loss = 3.0367951432466245\n",
            "iter 800 train loss = 3.041258506968571\n",
            "iter 900 train loss = 3.043065994708873\n",
            "iter 1000 train loss = 3.0459892639645023\n",
            "iter 1100 train loss = 3.0474154929534216\n",
            "iter 1200 train loss = 3.051233731318005\n",
            "iter 1300 train loss = 3.0529542326749866\n",
            "iter 1400 train loss = 3.053951945091273\n",
            "iter 1500 train loss = 3.054660309739432\n",
            "iter 1600 train loss = 3.0572007097977756\n",
            "iter 1700 train loss = 3.058730151431665\n",
            "iter 1800 train loss = 3.060646503358771\n",
            "iter 1900 train loss = 3.0614739301536633\n",
            "iter 2000 train loss = 3.0638376360884747\n",
            "Epoch 27 valid loss = 3.8881196842742876\n",
            "iter 0 train loss = 3.0352376302083335\n",
            "iter 100 train loss = 2.9870611598283068\n",
            "iter 200 train loss = 2.9973950815276593\n",
            "iter 300 train loss = 3.0113350158817203\n",
            "iter 400 train loss = 3.0152002245952865\n",
            "iter 500 train loss = 3.0184619672132964\n",
            "iter 600 train loss = 3.0239896507769726\n",
            "iter 700 train loss = 3.025952213771299\n",
            "iter 800 train loss = 3.0291208342020655\n",
            "iter 900 train loss = 3.03345415266258\n",
            "iter 1000 train loss = 3.0359355162090673\n",
            "iter 1100 train loss = 3.036837464677939\n",
            "iter 1200 train loss = 3.0398987528625714\n",
            "iter 1300 train loss = 3.0421901691203814\n",
            "iter 1400 train loss = 3.045173459393748\n",
            "iter 1500 train loss = 3.0466687042141456\n",
            "iter 1600 train loss = 3.0483681936628497\n",
            "iter 1700 train loss = 3.051058768000952\n",
            "iter 1800 train loss = 3.0531747661318747\n",
            "iter 1900 train loss = 3.0559984898550576\n",
            "iter 2000 train loss = 3.0580660833258135\n",
            "Epoch 28 valid loss = 3.8979841351922606\n",
            "iter 0 train loss = 2.989611030805229\n",
            "iter 100 train loss = 2.9961685421841544\n",
            "iter 200 train loss = 3.00302696357164\n",
            "iter 300 train loss = 3.0144782762009994\n",
            "iter 400 train loss = 3.0122585433893336\n",
            "iter 500 train loss = 3.015291851834342\n",
            "iter 600 train loss = 3.018061710660289\n",
            "iter 700 train loss = 3.0217766803977764\n",
            "iter 800 train loss = 3.025443340924925\n",
            "iter 900 train loss = 3.027613952896882\n",
            "iter 1000 train loss = 3.0292005822607417\n",
            "iter 1100 train loss = 3.032633675536203\n",
            "iter 1200 train loss = 3.035895194677597\n",
            "iter 1300 train loss = 3.038281632488282\n",
            "iter 1400 train loss = 3.0410500436058125\n",
            "iter 1500 train loss = 3.04264890073314\n",
            "iter 1600 train loss = 3.044395103623758\n",
            "iter 1700 train loss = 3.0463034222334446\n",
            "iter 1800 train loss = 3.0491288958521037\n",
            "iter 1900 train loss = 3.050575926947912\n",
            "iter 2000 train loss = 3.0526063054143546\n",
            "Epoch 29 valid loss = 3.89770048953584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXLps28_Clox",
        "colab_type": "text"
      },
      "source": [
        "## You present here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDNT4TRyClox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7a69a94-b3c7-4eac-83d0-c422832735f1"
      },
      "source": [
        "# check pdf to see what you expected to present\n",
        "model_pt = torch.load(\"transformer_model_best_5.pt\")\n",
        "plot_cache = model_pt['plot_cache']\n",
        "model = seq2seq(model_pt['opts'])\n",
        "model.load_state_dict(model_pt[\"state_dict\"])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkEMyqmvL9rz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0f2a443-19b3-405e-fcf4-37d9dbb4a2ba"
      },
      "source": [
        "!md5sum \"transformer_model_best_5.pt\""
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9a6346efc2ac4cb967f1c034dd30ab26  transformer_model_best_5.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68-tIxbBClo0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "26cd04b7-1d8e-4c37-ec05-b38acd92ff56"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
        "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Loss curves')\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjjKQpWyClpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "2516c34e-4f79-4990-cb22-b8dd180397b9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves')\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILJKvA4dLZdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6076552-82b6-4cbc-f5ed-2e9a2519cebe"
      },
      "source": [
        "print(\"Val PPL:\",2**(plot_cache[-1][1]/numpy.log(2)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val PPL: 42.51510228453153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR8qFUmsMvVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}